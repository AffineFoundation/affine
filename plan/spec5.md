Implementation Specification for Affine Reboot
Overview and Scope

This specification describes how to reâ€‘implement the Affine subnet on the Bittensor network. The goal is to create a minimal, verifiable headâ€‘toâ€‘head evaluation platform that fairly compares miners (AI models) using procedurally generated tasks and sets their Bittensor weights based on rigorous statistical evidence. The design removes legacy components such as the old AgentGym, Pareto frontier logic and roundâ€‘robin tournaments; instead it builds on principles from recent research (Ridges) and recommendations from the Bittensor validator documentation. Key goals include deterministic reproduction of every sample, simple and auditable code, robust exploit resistance and a clear winnerâ€‘takesâ€‘all incentive mechanism.

Core design principles

Determinism and reproducibility â€“ Every task instance (challenge) is defined by a challenge_id. Given the environment version and challenge ID, any validator can regenerate the exact task, reâ€‘run the modelâ€™s actions and verify the result. This mirrors Bittensorâ€™s emphasis that validators must be able to compute minersâ€™ scores offline
docs.taostats.io
.

Minimalism â€“ Keep the codebase lean: few files, short functions and no unnecessary dependencies. Use Pythonâ€™s standard library plus Gymnasium for environments. Avoid external frameworks and network calls in environments.

Exploit resistance â€“ Use blind procedural generation with seeded PRNGs, cryptographic hash chains for validator evidence and commitâ€“reveal for weight publication
docs.taostats.io
. Only deterministic judging is allowed; there is no subjective scoring.

Singleâ€‘champion duels â€“ The subnet keeps a single â€œchampionâ€ miner. Any newly registered miner becomes the â€œcontenderâ€ and is evaluated headâ€‘toâ€‘head against the champion. There is no full roundâ€‘robin tournament, which drastically reduces attack surface and cost.

Statistical stopping rules â€“ Use the oneâ€‘sided Wilson score interval to decide when a contender has convincingly outperformed the champion (or vice versa). This provides early stopping guarantees with a configurable confidence level
itl.nist.gov
.

1. Repository Layout

Delete the existing Affine modules (AgentGym, Pareto scoring, roundâ€‘robin sampling, outdated environments) and replace them with a small, clear package structure:

affine/
  core/
    ids.py           # Challenge ID spec and blake3 hashing
    prng.py          # Stateless PRNG (NumPy PCG64) seeded from challenge_id
    types.py         # dataclasses for Challenge, Sample, Verdict, BlockHeader etc.
    judge.py         # Deterministic judging utilities shared across envs
    wilson.py        # Oneâ€‘sided Wilson confidence interval helper
  envs/
    base.py          # Gymnasium adapter: reset(seed)->obs, step(action), verify()
    tictactoe.py     # Ticâ€‘Tacâ€‘Toe environment (multiâ€‘turn, miner vs miner)
    mult8.py         # 8â€‘digit multiplication environment (singleâ€‘turn)
    registry.py      # Registry mapping env name@version â†’ constructor
  duel/
    arena.py         # Implements contenderâ€‘vsâ€‘champion loop per environment
    aggregate.py     # Crossâ€‘environment aggregation and early stopping
  validator/
    sampleplan.py    # Commitâ€“reveal schedule for independent sampling
    bucket.py        # Appendâ€‘only block store (hash chain) + HTTP/S3 adapter
    vtrust.py        # Compute validator trust from overlap accuracy
    weights.py       # Compute winnerâ€‘takesâ€‘all weights and submit on chain
  miner/
    client.py        # Thin HTTP client to call a minerâ€™s chute (Chutes UID)
  net/
    bittensor.py     # Set weights; wraps Bittensor SDK
    chutes.py        # Invoke remote miner via Chutes; capture request_id
  cli.py             # Entry points: `af v2 validate`, `af v2 duel`, etc.
  pyproject.toml

Notes on structure

All modules live under affine/; no topâ€‘level scripts except cli.py.

Do not reintroduce the old AgentGym or Pareto dominance logic. The duel engine entirely replaces roundâ€‘robin tournaments and Pareto frontiers.

The envs directory houses only deterministic, Gymnasiumâ€‘compatible environments. Future tasks can be added here following the same pattern.

The validator package encapsulates all onâ€‘chain and network interactions: sampling, evidence storage, trust estimation and weight submission.

2. Environments
2.1 Common interface (envs/base.py)

All environments extend a minimal wrapper class that adapts Faramaâ€™s Gymnasium API. This wrapper defines a contract used by validators and miners:

class AffineEnv(gym.Env):
    """Base class for deterministic Affine environments."""
    metadata: dict = {'render_modes': [], 'name': '', 'version': ''}

    def reset(self, seed: int | None = None, options: dict | None = None)
            -> tuple[Observation, dict]:
        """Reset the environment using a seed (derived from challenge_id).
        Returns an initial observation and an info dict with at least:
            - challenge_id: hex or base58 string
            - env: name@version
            - spec_hash: blake3 hash of the environment code/parameters
            - ground_truth_commitment: hash used for verifying transcripts
        """

    def step(self, action: Any)
            -> tuple[Observation, float, bool, bool, dict]:
        """Advance the environment given an agent action.  Returns
        observation, reward, terminated, truncated, info.  The info may
        include debug data but must not leak ground truth."""

    def verify(self, response: str, info: dict) -> Verdict:
        """Pure function that checks a minerâ€™s response against ground truth.
        It must be deterministic given (challenge_id, env version) and not
        depend on any global state or network calls."""


Key requirements:

Deterministic seeding â€“ The wrapper uses a stateless PRNG (numpy.random.PCG64) seeded from a 64â€‘bit integer derived by hashing (env_idâ€–challenge_idâ€–version) using blake3. A validator can recompute the seed from challenge_id to regenerate the same task. The Procgen benchmark uses a similar pattern for reproducible procedurally generated games.

Metadata â€“ The info dict returned by reset() must include challenge_id, env (name@version), difficulty (if applicable), spec_hash (hash of the environment code and parameters) and ground_truth_commitment (cryptographic commitment used by validators to check transcripts). Inclusion of these fields allows external parties to verify that validators have executed the task correctly.

No network calls â€“ All randomness and ground truth are derived from the seed; the environment must not query external APIs or the internet. This prevents leakage of ground truth or injection of side channels.

2.2 Ticâ€‘Tacâ€‘Toe environment (tictactoe.py)

Purpose: Provide a simple multiâ€‘turn game where miners must act sequentially. The environment pairs the contender and champion against each other on a 3Ã—3 Ticâ€‘Tacâ€‘Toe board.

Key details:

Observation space: A flat 9â€‘cell board plus a binary indicator of which player moves next (X or O). Represent as a simple array or string ('XOXâ€¦').

Action space: Discrete(9) selecting a cell index. Invalid moves result in immediate loss.

Procedural start position: Optionally generate a partially filled board using the PRNG. In version 1.0 this can be left empty for simplicity.

Opponent logic: The side not controlled by the miner plays a perfect minimax strategy. Implement minimax in â‰¤80 lines; this ensures deterministic behaviour and a defined ground truth. The reward is +1 for a win, 0 for a draw and âˆ’1 for a loss. Draws do not count as wins for the Wilson interval.

Verification: The verify() method reconstructs the seed from challenge_id, rebuilds the game tree, replays the minerâ€™s actions and checks if the reported winner is correct. This ensures transcripts are unambiguous.

2.3 8â€‘Digit Multiplication environment (mult8.py)

Purpose: Provide a singleâ€‘turn arithmetic task to test symbolic reasoning.

Observation: Two eightâ€‘digit integers A and B encoded as strings or lists of digits.

Prompt (for miners): Compute A Ã— B; return only the integer.

Action space: The miner returns a string representing the product. In a languageâ€‘model setting, the entire textual response may contain other tokens; the verifier extracts the last integer token sequence from the reply and compares it to the ground truth.

Reward: +1 if the extracted integer matches exactly A * B, otherwise 0. Both being correct or both being wrong counts as a tie and does not increment the sample count.

Verification: The verify() implementation parses the minerâ€™s reply robustly (strip whitespace, handle separators) and performs the multiplication exactly using Pythonâ€™s bigâ€‘integer semantics. All ground truth is derived from the seed; no answer is embedded in the prompt.

2.4 Adding future environments

New environments follow the same pattern: derive all randomness from the seed, output a challenge ID and metadata, implement a deterministic verify() and avoid side effects. Each environment is versioned (name@semver); changing the deterministic logic requires bumping the version and updating the spec_hash.

3. Duel Evaluation and Statistical Decision

Affine uses a kingâ€‘ofâ€‘theâ€‘hill duel system: at any time there is a single champion miner. When a new miner registers, it becomes the contender and must beat the champion across multiple environments to take the crown.

3.1 Wilson score interval for perâ€‘environment decision

For each environment, the validator tracks the contenderâ€™s wins (w) and total valid trials (n) against the champion. Ties (both correct or both wrong) are excluded. After each sample, compute the lower bound of the oneâ€‘sided Wilson confidence interval (confidence level 95Â % by default). The NIST engineering statistics handbook gives the closedâ€‘form formula for the Wilson interval
itl.nist.gov
:

ğ¿
=
ğ‘
^
+
ğ‘§
2
2
ğ‘›
âˆ’
ğ‘§
ğ‘
^
(
1
âˆ’
ğ‘
^
)
ğ‘›
+
ğ‘§
2
4
ğ‘›
2
1
+
ğ‘§
2
ğ‘›
,
L=
1+
n
z
2
	â€‹

p
^
	â€‹

+
2n
z
2
	â€‹

âˆ’z
n
p
^
	â€‹

(1âˆ’
p
^
	â€‹

)
	â€‹

+
4n
2
z
2
	â€‹

	â€‹

	â€‹

,

where 
ğ‘
^
=
ğ‘¤
/
ğ‘›
p
^
	â€‹

=w/n and 
ğ‘§
z is the standard normal quantile for the desired confidence level (e.g., 1.96 for 95Â %). The algorithm stops when either:

L â‰¥ R_env â€“ The contender is confidently better on this environment (a win).

1 âˆ’ L â‰¥ R_env (equivalently, the contenderâ€™s upper bound â‰¤Â 1Â âˆ’Â R_env) â€“ The champion holds the environment.

A maximum number of samples (n_cap) is reached â€“ result is inconclusive.

The perâ€‘environment target R_env starts at 0.5 + Îµ (e.g., 0.51) and is adjusted by the global ratioâ€‘toâ€‘beat schedule described below.

Pseudoâ€‘code for one environment (ties are dropped):

def duel_env(contender, champion, env, R_env=0.5+eps, alpha=0.05, n_cap=2000):
    w = n = 0
    for challenge_id in generate_challenges(env):
        winner = play_one_game(contender, champion, env, challenge_id)
        if winner == 'contender':
            w += 1; n += 1
        elif winner == 'champion':
            n += 1
        # ties do not increment n
        if n >= 1:
            p = w / n
            L = wilson_lower_bound(p, n, alpha)
            U = 1 - wilson_lower_bound(1-p, n, alpha)
            if L >= R_env:
                return 'win', w, n, L, U
            if U <= 1 - R_env:
                return 'loss', w, n, L, U
        if n >= n_cap:
            return 'undecided', w, n, L, U

3.2 Multiâ€‘environment aggregation

Let there be E environments in the registry. The validator runs the duel loop in parallel across all environments. Maintain counts W (number of environments the contender has won) and L (lost). After each environment resolves to win or loss, update these counts and check the global stopping rule:

Define R_glob as the current global ratioâ€‘toâ€‘beat (0.5 + Îµ initially). The contender must win at least ceil(R_glob * E) environments to become champion.

If W â‰¥ ceil(R_glob * E), stop early â€“ the contender dethrones the champion.

If W + (E âˆ’ W âˆ’ L) < ceil(R_glob * E), the contender cannot possibly reach the target â€“ stop and keep the champion.

Otherwise, continue evaluating remaining environments until all are decided or until further early stopping is possible.

This early stopping reduces cost: e.g., if the contender loses the first two environments out of three and R_glob = 0.51, they cannot reach ceil(0.51 * 3) = 2 wins; evaluation can stop immediately.

3.3 Ratioâ€‘toâ€‘beat schedule (difficulty ratchet)

Using a constant 0.5 threshold makes trivial dethronements too easy (e.g., a miner winning 51Â % by chance). Instead, after each dethronement, update the ratio to beat:

For each environment where the contender won, compute a regularized win rate: r_e = (wins_e + 1) / (losses_e + 1).

Take the geometric mean 
ğ‘Ÿ
Ë‰
r
Ë‰
 of these r_e values across winning environments.

Map 
ğ‘Ÿ
Ë‰
r
Ë‰
 to a probability threshold:

ğ‘…
new
=
ğ‘Ÿ
Ë‰
1
+
ğ‘Ÿ
Ë‰
.
R
new
	â€‹

=
1+
r
Ë‰
r
Ë‰
	â€‹

.

If 
ğ‘Ÿ
Ë‰
=
1
r
Ë‰
=1 (equal performance), this yields 0.5; if 
ğ‘Ÿ
Ë‰
>
1
r
Ë‰
>1 (better performance), the new ratio is >0.5.

Set R_glob to 0.5 + (R_new âˆ’ 0.5). Store the timestamp of this update.

Apply exponential decay: at each validator epoch, update R_glob:

ğ‘…
glob
(
ğ‘¡
)
=
0.5
+
(
ğ‘…
peak
âˆ’
0.5
)
â€‰
ğ‘’
âˆ’
(
ğ‘¡
âˆ’
ğ‘¡
peak
)
/
ğœ
,
R
glob
	â€‹

(t)=0.5+(R
peak
	â€‹

âˆ’0.5)e
âˆ’(tâˆ’t
peak
	â€‹

)/Ï„
,

where R_peak is the ratio after the last dethronement and 
ğœ
Ï„ is the halfâ€‘life in epochs (e.g., 14). Decay prevents lockâ€‘in; over time the ratio drifts back to 0.5 so a new miner has a chance.

4. Validator Workflow and Evidence

Validators independently sample the miner pair (contender vs champion), record full transcripts of each challenge and publish these transcripts in verifiable blocks. They then crossâ€‘validate each otherâ€™s evidence and set weights on chain using a winnerâ€‘takesâ€‘all rule.

4.1 Sampling plan with commitâ€“reveal

At the start of each epoch, each validator generates a secret seed s_v,t and publishes its commitment C_v,t = blake3(s_v,t). After a predetermined delay (one epoch), the validator reveals s_v,t, allowing others to derive the exact sequence of challenge_ids used for sampling. This commitâ€“reveal schedule prevents cherryâ€‘picking: validators cannot retroactively choose favourable tasks because they must sample according to their published plan. The commitâ€“reveal mechanism is similar to Bittensorâ€™s Commit Reveal 3.0 used to deter weight copyingâ€”weights are encrypted and only revealed after several epochs
docs.taostats.io
docs.taostats.io
.

Given a revealed seed, derive challenge IDs for each environment:

cid
ğ‘£
,
ğ‘¡
,
ğ‘–
,
ğ‘’
=
blake3
(
ğ‘ 
ğ‘£
,
ğ‘¡
âˆ¥
epoch_anchor
ğ‘¡
âˆ¥
ğ‘’
âˆ¥
ğ‘–
)
,
cid
v,t,i,e
	â€‹

=blake3(s
v,t
	â€‹

âˆ¥epoch_anchor
t
	â€‹

âˆ¥eâˆ¥i),

where epoch_anchor_t is a public randomness beacon (e.g., a recent Bittensor or Bitcoin block hash), e is the environment name and i is a perâ€‘env counter. Validators must sample tasks in the order determined by this function; any offâ€‘schedule samples are ignored or penalised.

4.2 Recording samples and building blocks

For each challenge, the validator performs the following:

Generate challenge_id using the commit plan.

Call the contender and champion miners via Chutes (the inference gateway). Capture a request_id or invocation identifier if provided by Chutes.

Record a Sample object containing:

env: name@version
challenge_id: str
miner_uid: int
role: 'contender' | 'champion'
prompt: str
response: str
info: dict   # from env.reset(); includes metadata, spec_hash, etc.
verdict: Verdict  # ok/ko and reason from env.verify()
timing: {latency_ms: float, tokens: int}
request_id: Optional[str]


Append samples to an inâ€‘memory list; when a batch size (e.g., 100 samples) is reached, construct a Block.

A BlockHeader contains:

prev_hash: str
height: int
created_at: int
validator: hotkey
epoch: int
sample_count: int
merkle_root: str  # root of sample hashes
signature: str    # validator signs header


Hash the concatenation of header and sample hashes using blake3 or SHAâ€‘256 to produce the block hash. Store the block (header + list of sample hashes) locally and push it to a shared bucket (e.g., HTTP/S3/IPFS). Each validator maintains their own appendâ€‘only chain; the prev_hash field forms a tamperâ€‘evident log.

4.3 Crossâ€‘pull and validator trust (VTrust)

Validators fetch each otherâ€™s blocks from the shared bucket, recompute the hashes and verify the signatures. For each sample, they independently recompute the ground truth using verify() and compare it to the posted verdict. Based on the overlap, compute each validatorâ€™s VTrust scoreâ€”the fraction of correctly reported samples weighted by prior evidence. Validators whose verdicts frequently disagree with the recomputed truth or whose blocks are invalid lose trust. The Bittensor docs describe VTRUST as a validatorâ€™s trust score showing the networkâ€™s trust in its reports
docs.learnbittensor.org
. A validator with low trust has little influence on the final weight setting.

4.4 Weight setting (winner takes all)

After aggregating all validatorsâ€™ evidence (weighted by their VTrust), compute the global duel result. If the contender wins, set their weight to 1.0 and all other minersâ€™ weights to 0.0. If the champion holds, keep weights unchanged. The commitâ€‘reveal mechanism ensures that validators cannot copy weights: weights are committed to the chain but only revealed after several epochs
docs.taostats.io
. Validators must use the Bittensor SDK (bittensor.set_weights) or the CLI to submit weights within each tempo.

5. Data Types and Helper Functions
5.1 Dataclasses (core/types.py)

Use Python @dataclass definitions for clarity and immutability:

@dataclass(frozen=True)
class Challenge:
    env: str         # env name@version
    challenge_id: str
    meta: dict       # includes difficulty, spec_hash etc.

@dataclass(frozen=True)
class Verdict:
    ok: bool
    reason: str = ''

@dataclass(frozen=True)
class Sample:
    env: str
    challenge_id: str
    miner_uid: int
    role: str   # 'contender' or 'champion'
    prompt: str
    response: str
    info: dict
    ok: bool
    reason: str
    request_id: Optional[str] = None

@dataclass(frozen=True)
class BlockHeader:
    prev_hash: str
    height: int
    created_at: int
    validator: str
    epoch: int
    sample_count: int
    merkle_root: str
    signature: str

5.2 Wilson interval helper (core/wilson.py)

Implement a small function to compute the lower bound of the oneâ€‘sided Wilson interval. Input (wins, n, alpha) outputs a float. Use the formula from NIST
itl.nist.gov
. Keep the function ~10 lines.

5.3 Duel API (duel/arena.py and duel/aggregate.py)

arena.py implements duel_env(contender_uid, champion_uid, env, R_env, alpha) following the pseudoâ€‘code in Â§3.1. It should yield the result (win, loss or undecided) and the statistics used. aggregate.py implements duel_many_envs(contender_uid, champion_uid, env_names, R_glob, ...) to coordinate multiple environments, apply early stopping and update R_glob when the contender wins.

5.4 Validator utilities (validator/sampleplan.py, bucket.py, vtrust.py, weights.py)

sampleplan.py â€“ Manage commitâ€“reveal seeds, derive challenge IDs, publish commitments and reveals.

bucket.py â€“ Provide a filesystemâ€‘like interface to read and write block files; support different backâ€‘ends (local disk, HTTP, S3). Ensure canonical JSON encoding before hashing.

vtrust.py â€“ Compute each validatorâ€™s trust as a Betaâ€‘prior updated fraction of correct vs incorrect verdicts. Validators with too many invalid or offâ€‘schedule samples get penalized.

weights.py â€“ Collect weighted votes across validators and call net.bittensor.set_weights() to update miner weights.

6. Determinism & Verification Details

Challenge IDs: Represent as 128â€‘bit hex or base58 strings. Derive seeds by hashing (env_idâ€–challenge_idâ€–version) and taking the first 64Â bits. Use blake3 or blake2s for hashing; these functions produce a uniformly distributed random seed.

Reproducibility: Third parties with (env, version, challenge_id, transcript) can fully regenerate the environment, replay the minerâ€™s actions and verify the verdict using the openâ€‘source code. This transparency is central to verifiability and trust.

Transcript format: Store transcripts as arrays of steps with fields t (turn number), role (env, minerA, minerB), content (prompt/response), obs_summary (optional summarised observation) and action_summary. Truncate or hash extremely long responses to a fixed length and include a hash footer for auditability.

No hidden state: All environment state must be reconstructible from the seed and the action sequence. Validators must not rely on hidden caches or remote services.

7. Antiâ€‘Cheat Measures and Failure Modes

To maintain fairness, incorporate the following protections:

No answer leakage: Prompts must not include the correct answer (e.g., the mult8 environment asks only for the product, not verifying within the prompt). The ground truth stays on the validator side.

Timing and bandwidth signals: Record latency (ms) and response length (tokens) for each miner. Abnormally low latencies may indicate caching; extremely long latencies may indicate timeouts. Use these signals for additional trust heuristics (e.g., penalise responses that arrive after the time budget).

Duplicate detection: Each (env, challenge_id, miner_uid) combination should be unique within a sliding window. Duplicate submissions may indicate replay attacks; duplicates are deâ€‘weighted when computing trust and win rates.

Block tamper detection: Invalid block signatures, broken prev_hash chains or mismatched merkle roots cause the block to be ignored and the validatorâ€™s trust to drop.

Commitâ€“reveal enforcement: Validators must sample according to their published plan. Offâ€‘schedule samples are discarded and counted as incorrect for VTrust purposes. Commitâ€“reveal prevents weight copying by delaying the publication of weights
docs.taostats.io
.

Version pinning: Each environment has a semver version number. Changing the environment logic requires a version bump so that old challenge IDs cannot be reinterpreted.

8. Migration and Rollâ€‘Out Plan

Freeze current Affine repository. Create a branch v2/cleanroom and stop development on the legacy code. All new work occurs on this branch.

Delete legacy modules (AgentGym, Pareto evaluation, roundâ€‘robin sampling, old environments). Remove any dependencies or build targets related to the old design.

Implement environments first. Create affine/envs/base.py, tictactoe.py, mult8.py and ensure determinism and verify() correctness. Add property tests to verify that the same challenge_id produces identical tasks and outcomes.

Implement the duel engine. Write core/wilson.py for Wilson confidence intervals, duel/arena.py for perâ€‘environment duels and duel/aggregate.py for multiâ€‘environment aggregation and ratioâ€‘toâ€‘beat updates. Add tests with synthetic Bernoulli streams to ensure correct stopping behaviour.

Implement validator tooling. Write the sampling plan commitâ€“reveal logic, block construction and trust estimation. Integrate with Chutes (SN64) to query miners, capturing request_id when possible. Use Bittensorâ€™s set_weights API to submit weights.

Build a minimal CLI. Provide subcommands: af v2 validate (run validator loop and publish blocks), af v2 duel (run a oneâ€‘off duel locally), af v2 env run (generate and inspect challenges for debugging), af v2 commit-plan and af v2 reveal-plan (publish and reveal sampling seeds), af v2 set-weights (recompute winner and set weights on chain).

Perform a testnet dry run. Run validators and miners on a test subnet without setting onâ€‘chain weights. Observe the duel outcomes, block propagation and ratioâ€‘toâ€‘beat behaviour. Fineâ€‘tune defaults (e.g., eps, n_cap, Ï„) based on initial results.

Enable onâ€‘chain weights. After testing, allow validators to submit weights in commitâ€“reveal mode. Monitor network metrics (VTrust, emissions, churn) and adjust parameters if necessary. Publish documentation and guidelines for miners and validators.

9. Testing Strategies

Environment determinism: Generate thousands of seeds and verify that env.reset(seed) returns identical observations and ground truth across multiple runs and machines.

Verifier invariants: For a recorded transcript, reâ€‘run verify() and ensure the verdict matches exactly. Mutate transcripts deliberately to check that invalid transcripts are detected.

Wilson harness property tests: Simulate Bernoulli streams with known probabilities. Confirm that the Wilson test accepts with high probability when the true success rate â‰¥ target and rejects when the true rate is below the target.

Endâ€‘toâ€‘end duel tests: Create dummy miners: one returns correct answers with probability p (e.g., 0.55), the other random. Confirm that the contender dethrones the champion when p exceeds the ratioâ€‘toâ€‘beat and that the ratio ratchets upward and then decays over epochs.

Block integrity tests: Tamper with sample hashes or block signatures and verify that crossâ€‘validators detect the tampering and reduce VTrust.

10. Defaults and Configuration Parameters

Confidence level: 95Â % (Î±Â =Â 0.05).

Îµ (initial slack): 0.01, so R_glob = 0.51 by default.

n_cap: 2Â 000 samples per environment (prevents endless evaluation).

Halfâ€‘life Ï„: 14 epochs (ratioâ€‘toâ€‘beat decays back to 0.5 roughly every two weeks).

Block size: 100 samples; adjust to trade off latency vs overhead.

Timeouts: 2Â s per move in Ticâ€‘Tacâ€‘Toe; 10Â s total for Mult8; 30Â s maximum request time per sample. Timeouts count as losses.

VTrust prior: Beta(Î±=1, Î²=1) (Laplace prior) for computing trust from correct/incorrect counts.

11. Summary

This specification charts a complete overhaul of Affine. By adopting deterministic Gymnasium environments, a championâ€“contender duel system, rigorous Wilsonâ€‘score stopping rules, commitâ€“reveal sampling and verifiable block logs, the new Affine ensures fairness, reproducibility and resistance to known attack vectors. Validators sample tasks independently yet share evidence via tamperâ€‘evident chains; winners are determined collectively and published via onâ€‘chain weights. When fully implemented, this design will produce a lowâ€‘lineâ€‘count codebase that is easy to audit and maintain while aligning with Bittensorâ€™s validator model and the broader research trends in AI incentive design.
