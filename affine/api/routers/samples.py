"""
Sample Results Router

Endpoints for submitting and querying sample results.
"""

import time
import uuid
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, Request, Query, status
from affine.api.models import (
    SampleSubmitRequest,
    SampleSubmitResponse,
    SampleListResponse,
    SampleFullResponse,
    SampleDetail,
    PaginationInfo,
    ScorerSampleData,
    ScorerMinerSamplesResponse,
)
from affine.api.dependencies import (
    get_sample_results_dao,
    verify_signature_dependency,
    rate_limit_read,
    rate_limit_write,
)
from affine.api.utils.pagination import get_pagination_params, create_pagination_info
from affine.database.dao.sample_results import SampleResultsDAO
from affine.core.miners import miners as get_miners

router = APIRouter(prefix="/samples", tags=["Samples"])


@router.post("", response_model=SampleSubmitResponse, dependencies=[Depends(rate_limit_write)])
async def submit_sample(
    request: Request,
    sample: SampleSubmitRequest,
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Submit a sample result (requires signature verification).
    
    The request must include:
    - X-Hotkey header: Miner's hotkey
    - X-Signature header: Signature of the request body
    
    The signature should be generated by signing the JSON payload with the miner's private key.
    """
    # Verify signature
    hotkey = await verify_signature_dependency(request)
    
    # Verify hotkey matches the sample
    if hotkey != sample.miner_hotkey:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Hotkey in signature does not match miner_hotkey in payload"
        )
    
    # Save sample to database
    try:
        await dao.save_sample(
            miner_hotkey=sample.miner_hotkey,
            model_revision=sample.model_revision,
            model=sample.model,
            env=sample.env,
            task_id=sample.task_id,
            score=sample.score,
            latency_ms=sample.latency_ms,
            extra=sample.extra.model_dump(),
            validator_hotkey=sample.validator_hotkey,
            block_number=sample.block_number,
            signature=request.headers.get("X-Signature", ""),
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save sample: {str(e)}"
        )
    
    return SampleSubmitResponse(
        sample_id=sample.task_id,  # Return task_id as identifier
        created_at=int(time.time()),
        message="Sample saved successfully"
    )


@router.get("/miner/{hotkey}", response_model=SampleListResponse, dependencies=[Depends(rate_limit_read)])
async def get_miner_samples(
    hotkey: str,
    model_revision: str = Query(..., description="Model revision (required)"),
    env: str = Query(..., description="Environment name (required, e.g., affine:sat)"),
    limit: int = Query(100, description="Maximum number of results", ge=1, le=1000),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get samples for a specific miner, revision, and environment.
    
    Required query parameters:
    - model_revision: Model revision hash
    - env: Environment name (e.g., affine:sat, agentgym:webshop)
    
    Optional query parameters:
    - limit: Maximum results to return (default: 100, max: 1000)
    
    Note: env is required because it's part of the partition key.
    To query multiple environments, use /miner/{hotkey}/multi endpoint.
    """
    try:
        # Get samples from DAO
        samples = await dao.get_samples_by_miner(
            miner_hotkey=hotkey,
            model_revision=model_revision,
            env=env,
            limit=limit,
        )
        
        # Convert to response models
        sample_details = [
            SampleDetail(
                timestamp=s["timestamp"],
                env=s["env"],
                score=s["score"],
                latency_ms=s["latency_ms"],
                signature=s["signature"],
            )
            for s in samples
        ]
        
        # Create pagination info
        pagination = PaginationInfo(
            total=len(samples),
            limit=limit,
            next_cursor=None,
        )
        
        return SampleListResponse(
            samples=sample_details,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve samples: {str(e)}"
        )


@router.get("/miner/{hotkey}/multi", response_model=SampleListResponse, dependencies=[Depends(rate_limit_read)])
async def get_miner_samples_multi_env(
    hotkey: str,
    model_revision: str = Query(..., description="Model revision (required)"),
    envs: str = Query(..., description="Comma-separated environment names"),
    limit_per_env: int = Query(100, description="Maximum results per environment", ge=1, le=1000),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get samples for a miner across multiple environments.
    
    Required query parameters:
    - model_revision: Model revision hash
    - envs: Comma-separated environment names (e.g., "affine:sat,affine:abd,agentgym:webshop")
    
    Optional query parameters:
    - limit_per_env: Maximum results per environment (default: 100, max: 1000)
    """
    try:
        # Parse environments
        env_list = [e.strip() for e in envs.split(",")]
        
        # Get samples across all environments
        results_by_env = await dao.get_samples_by_miner_all_envs(
            miner_hotkey=hotkey,
            model_revision=model_revision,
            envs=env_list,
            limit_per_env=limit_per_env,
        )
        
        # Flatten results
        all_samples = []
        for env_samples in results_by_env.values():
            all_samples.extend(env_samples)
        
        # Convert to response models
        sample_details = [
            SampleDetail(
                timestamp=s["timestamp"],
                env=s["env"],
                score=s["score"],
                latency_ms=s["latency_ms"],
                signature=s["signature"],
            )
            for s in all_samples
        ]
        
        # Sort by timestamp descending
        sample_details.sort(key=lambda x: x.timestamp, reverse=True)
        
        pagination = PaginationInfo(
            total=len(sample_details),
            limit=limit_per_env * len(env_list),
            next_cursor=None,
        )
        
        return SampleListResponse(
            samples=sample_details,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve samples: {str(e)}"
        )


@router.get("/uid/{uid}", response_model=SampleListResponse, dependencies=[Depends(rate_limit_read)])
async def get_uid_samples(
    uid: int,
    envs: str = Query("affine:sat,affine:abd,affine:ded", description="Comma-separated environments"),
    limit_per_env: int = Query(10, description="Maximum results per environment", ge=1, le=1000),
    include_extra: bool = Query(False, description="Include extra field (conversation data, requires decompression)"),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get recent samples for a specific UID across multiple environments.
    
    This endpoint:
    1. Queries bittensor metagraph to get miner info (hotkey + model_revision)
    2. Queries samples across specified environments using hotkey+revision
    3. Returns combined results
    
    If the UID is not currently assigned or has no valid commit, returns empty list.
    
    Query parameters:
    - envs: Comma-separated environment names (default: affine:sat,affine:abd,affine:ded)
    - limit_per_env: Maximum results per environment (default: 10, max: 1000)
    - include_extra: Include extra field with conversation data (default: False)
      Setting to False saves bandwidth and decompression cost
    
    Note: model_revision is queried from bittensor commits, not provided by user.
    """
    try:
        # Query bittensor metagraph to get miner info (including model_revision)
        # Reference: affine.miners.miners() implementation
        miners_dict = await get_miners(uids=uid, check_validity=False)
        
        if not miners_dict or uid not in miners_dict:
            # UID not currently assigned or no valid commit
            return SampleListResponse(
                samples=[],
                pagination=PaginationInfo(total=0, limit=limit_per_env, next_cursor=None)
            )
        
        miner = miners_dict[uid]
        hotkey = miner.hotkey
        model_revision = miner.revision
        
        if not model_revision:
            # No revision in commit, cannot query samples
            return SampleListResponse(
                samples=[],
                pagination=PaginationInfo(total=0, limit=limit_per_env, next_cursor=None)
            )
        
        # Parse environments
        env_list = [e.strip() for e in envs.split(",")]
        
        # Get samples across all environments
        results_by_env = await dao.get_samples_by_miner_all_envs(
            miner_hotkey=hotkey,
            model_revision=model_revision,
            envs=env_list,
            limit_per_env=limit_per_env,
            include_extra=include_extra,
        )
        
        # Flatten results
        all_samples = []
        for env_samples in results_by_env.values():
            all_samples.extend(env_samples)
        
        # Convert to response models
        sample_details = [
            SampleDetail(
                timestamp=s["timestamp"],
                env=s["env"],
                score=s["score"],
                latency_ms=s["latency_ms"],
                signature=s["signature"],
                extra=s.get("extra") if include_extra else None,
            )
            for s in all_samples
        ]
        
        # Sort by timestamp descending
        sample_details.sort(key=lambda x: x.timestamp, reverse=True)
        
        pagination = PaginationInfo(
            total=len(sample_details),
            limit=limit_per_env * len(env_list),
            next_cursor=None,
        )
        
        return SampleListResponse(
            samples=sample_details,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve samples: {str(e)}"
        )


@router.get("/timestamp-range", response_model=SampleListResponse, dependencies=[Depends(rate_limit_read)])
async def get_samples_by_timestamp(
    start_time: int = Query(..., description="Start timestamp in milliseconds"),
    end_time: int = Query(..., description="End timestamp in milliseconds"),
    limit: int = Query(100, description="Maximum number of results", ge=1, le=1000),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get samples by timestamp range.
    
    Uses timestamp-index GSI for efficient time-based queries.
    
    Query parameters:
    - start_time: Start timestamp in milliseconds (required)
    - end_time: End timestamp in milliseconds (required)
    - limit: Maximum results to return (default: 100, max: 1000)
    """
    try:
        # Query using timestamp GSI
        samples = await dao.get_samples_by_timestamp_range(
            start_timestamp=start_time,
            end_timestamp=end_time,
            limit=limit,
        )
        
        # Convert to response models
        sample_details = [
            SampleDetail(
                timestamp=s["timestamp"],
                env=s["env"],
                score=s["score"],
                latency_ms=s["latency_ms"],
                signature=s["signature"],
            )
            for s in samples
        ]
        
        pagination = PaginationInfo(
            total=len(samples),
            limit=limit,
            next_cursor=None,
        )
        
        return SampleListResponse(
            samples=sample_details,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve samples: {str(e)}"
        )


@router.get("/scorer/{hotkey}", response_model=ScorerMinerSamplesResponse, dependencies=[Depends(rate_limit_read)])
async def get_scorer_miner_samples(
    hotkey: str,
    model_revision: str = Query(..., description="Model revision (required)"),
    env: str = Query(..., description="Environment name (required, e.g., affine:sat)"),
    dataset_length: int = Query(..., description="Total number of tasks in dataset", ge=1),
    task_id_start: Optional[int] = Query(None, description="Task ID range start (inclusive, for dataset expansion)"),
    task_id_end: Optional[int] = Query(None, description="Task ID range end (exclusive, for dataset expansion)"),
    deduplicate: bool = Query(True, description="Deduplicate by task_id, keeping latest sample"),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get samples for scorer with completion status.
    
    This endpoint is optimized for the scorer service:
    - Returns samples without conversation data (saves bandwidth)
    - Includes completion status (is_complete flag)
    - Supports Task ID range for dataset expansion transitions
    - Deduplicates by task_id by default (keeps latest sample)
    
    Required query parameters:
    - model_revision: Model revision hash
    - env: Environment name (e.g., affine:sat)
    - dataset_length: Total number of tasks in the dataset
    
    Optional query parameters:
    - task_id_start: Start of task ID range (inclusive), overrides dataset_length if both start and end provided
    - task_id_end: End of task ID range (exclusive), overrides dataset_length if both start and end provided
    - deduplicate: If True (default), keep only latest sample per task_id
    
    Example usage during dataset expansion:
    - Old dataset: 0-99 (dataset_length=100)
    - New dataset: 0-149 (dataset_length=150)
    - Transition period: use task_id_start=0, task_id_end=150
    """
    try:
        # Get samples with completion status
        result = await dao.get_samples_with_completion_status(
            miner_hotkey=hotkey,
            model_revision=model_revision,
            env=env,
            dataset_length=dataset_length,
            deduplicate_by_task_id=deduplicate,
            include_extra=False,  # Scorer doesn't need conversation data
            task_id_start=task_id_start,
            task_id_end=task_id_end,
        )
        
        # Convert to scorer-specific response format
        scorer_samples = [
            ScorerSampleData(
                task_id=s["task_id"],
                score=s["score"],
                latency_ms=s["latency_ms"],
                timestamp=s["timestamp"],
                block_number=s["block_number"],
            )
            for s in result["samples"]
        ]
        
        return ScorerMinerSamplesResponse(
            miner_hotkey=hotkey,
            model_revision=model_revision,
            env=env,
            samples=scorer_samples,
            is_complete=result["is_complete"],
            completed_count=result["completed_count"],
            total_count=result["total_count"],
            missing_task_ids=result["missing_task_ids"],
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve scorer samples: {str(e)}"
        )


@router.get("/{hotkey}/{env}/{task_id}", response_model=SampleFullResponse, dependencies=[Depends(rate_limit_read)])
async def get_sample(
    hotkey: str,
    env: str,
    task_id: str,
    model_revision: str = Query(..., description="Model revision"),
    dao: SampleResultsDAO = Depends(get_sample_results_dao),
):
    """
    Get a specific sample by its natural key components.
    
    Path parameters:
    - hotkey: Miner hotkey
    - env: Environment (e.g., affine:sat)
    - task_id: Task identifier
    
    Query parameters:
    - model_revision: Model revision hash
    
    Returns full sample details including conversation data.
    """
    try:
        # Construct PK and SK from natural keys
        pk = dao._make_pk(hotkey, model_revision, env)
        sk = dao._make_sk(task_id)
        
        # Get item directly using PK/SK
        from affine.database.client import get_client
        client = get_client()
        
        response = await client.get_item(
            TableName=dao.table_name,
            Key={
                'pk': {'S': pk},
                'sk': {'S': sk}
            }
        )
        
        if 'Item' not in response:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Sample not found for hotkey={hotkey}, env={env}, task_id={task_id}"
            )
        
        # Deserialize and decompress
        item = dao._deserialize(response['Item'])
        
        if 'extra_compressed' in item:
            import json
            compressed = item['extra_compressed']
            extra_json = dao.decompress_data(compressed)
            extra = json.loads(extra_json)
        else:
            extra = item.get('extra', {})
        
        return SampleFullResponse(
            miner_hotkey=item["miner_hotkey"],
            model_revision=item["model_revision"],
            env=item["env"],
            score=item["score"],
            signature=item["signature"],
            extra=extra,
            timestamp=item["timestamp"],
            block_number=item["block_number"],
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve sample: {str(e)}"
        )